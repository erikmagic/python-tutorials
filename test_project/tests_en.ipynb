{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tests et Profilage, Python\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The purpose of this tutorial is to clarify certain concepts related to testing in\n",
    "programming and to emphasize good insurance practices\n",
    "quality in Python. We will also investigate how to profile its code.\n",
    "to determine performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unit tests\n",
    "----------\n",
    "The main pillar of quality assurance in programming\n",
    "is unit tests. The purpose of unit tests is to verify that\n",
    "my code does what it's supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The unit tests consist of 4 steps:\n",
    "1. Preparation (`setUp`): the components needed for the test are\n",
    "   instantiated.\n",
    "2. Exercise: The unit of code being tested is launched.\n",
    "3. Verification: The output of the unit of code in question is compared to the output of its\n",
    "   anticipated value.\n",
    "4. TearDown: The components needed for the test are\n",
    "   destroyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un test unitaire est généralement conçu afin d'isoler une unité de code. Il\n",
    "est important de découpler les tests unitaires.   Nous allons commencer par\n",
    "nous habituer à la structure générale des tests. Le premier exemple est le\n",
    "script `utilities.py` et sa classe de tests dans `test_utilities.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TestUtilities:\n",
    "    \"\"\"\n",
    "    This class is a classic example of xUnit test cases.\n",
    "    The setup_class/teardown_class methods are called automatically at the beginning and end of unit tests belonging to this class.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def setup_class(cls):\n",
    "        cls.converter = StringConverters()\n",
    "\n",
    "    @classmethod\n",
    "    def teardown_class(cls):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def test_translate_to_jadensmith_raises_value_error(self):\n",
    "        with pytest.raises(ValueError):\n",
    "            self.converter.translate_to_jadensmith(999)\n",
    "\n",
    "    ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We get our usual test structure with a test class. Note\n",
    "that the class starts with the keyword `Test` which allows the pytest module to\n",
    "include it. In addition, the tests in the class all start with the `test_` prefix.\n",
    "for the same reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pytest allows us to run our tests and see the results.\n",
    "with a graphical summary on the command line. The command is `pytest <your_test_file.py> `."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are several useful options for getting started\n",
    "such as `-s` which displays the _standard out_ (the print statements, or else they're not printed since pytest captures all the outbounds tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest is one of the main test modules in Python. It supports\n",
    "classic units as we've just seen. In addition, it allows to\n",
    "handle tests differently with _fixture_. We will refactor the\n",
    "class TestUtilities to use the _fixture_.\n",
    "The file is *test_utilities_refactored.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@pytest.fixture(autouse=True, scope='module')\n",
    "def create_converter():\n",
    "    converter = StringConverters()\n",
    "    yield converter\n",
    "\n",
    "def test_translate_to_jadensmith_raises_value_error(create_converter):\n",
    "    with pytest.raises(ValueError):\n",
    "        create_converter.translate_to_jadensmith(999)\n",
    "\n",
    "def test_translate_to_jadensmith_single_word(create_converter):\n",
    "    sentence = \"hello\"\n",
    "    expected_value = \"Hello\"\n",
    "\n",
    "    actual_value = create_converter.translate_to_jadensmith(sentence)\n",
    "\n",
    "    assert expected_value == actual_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The _fixtures_ are a very powerful feature of pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pytest Coverage\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_pytest-cov_ is an additional module allowing us to visualize the\n",
    "proportion of the code tested. It is easy to narrow down to just a few possible paths through the code.\n",
    "For example, if I have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if first_name == \"\":\n",
    "   if last_name == \"\":\n",
    "        db.set_name(\"unknown\")\n",
    "   else:\n",
    "        db.set_name(last_name)\n",
    "else:\n",
    "    if last_name == \"\":\n",
    "        db.set_name(first_name + \"_unknown\")\n",
    "    else:\n",
    "        db.set_name(first_name +  \" \" + last_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have to write either 4 tests or one test that goes all the way.\n",
    "A module like _pytest-cov_ allows us to compute the proportion of the code which is actually tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The command is `pytest --cov=<module name> <test_directory>`\n",
    "\n",
    "![pytest cov result](./include/pytest_cov.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The previous picture shows the result of a test coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Benchmarking in Python\n",
    "----------------------\n",
    "Sometimes it is important to meet certain performance criteria.\n",
    "How to validate the performance of specific components of our software?\n",
    "For isolated functions, we can use the `timeit` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "s = \"\"\"\n",
    "sc = StringConverters()\n",
    "with open(\"sherlock.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "meme_lines = [sc.translate_to_internet_meme(l) for l in lines[:100]]\n",
    "\"\"\"\n",
    "print(timeit.timeit(stmt=s, setup=\"from __main__ import  StringConverters\", number=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The previous code opens a text file (Sherlock Holmes) and calls the `translate_to_internet_meme` function. \n",
    "on the first 100 sentences. This procedure is repeated 1000 times. This results in \n",
    "a fairly accurate performance average.   \n",
    "Note that the code is in `str` form which limits the testing possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Profiling en Python\n",
    "-------------------\n",
    "In addition to benchmarking, profiling can also be useful in order to better understand one's program.\n",
    "Profiling is a set of descriptive statistics on the execution time of the various components of a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most popular and useful module is probably the `cprofile` module.\n",
    "This module is designed to create a runtime profile, not for benchmarking purposes, use `timeit` in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The command to run the *cprofile* profiler is as follows:\n",
    "`python -m cprofile <your_script_name.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def f(x):\n",
    "    x = str(x)\n",
    "    c = \"this value is \" + x +  \".\"\n",
    "    time.sleep(1)\n",
    "    print(c)\n",
    "\n",
    "l = [1,2,3,4,5,6,7]\n",
    "\n",
    "for v in l:\n",
    "    f(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![cprofile](./include/result_cprofile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the meaning of the columns:\n",
    "* ncalls: the number of times the function has been called.\n",
    "* tottime: the total time spent in a function.\n",
    "* percall: tottime / ncalls\n",
    "* cumtime: cumulative time spent in a function and its sub-functions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
